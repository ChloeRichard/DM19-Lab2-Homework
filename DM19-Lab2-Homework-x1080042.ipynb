{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Chloé RICHARD\n",
    "\n",
    "Student ID: X1080042\n",
    "\n",
    "GitHub ID: ChloeRichard\n",
    "\n",
    "Kaggle name: chlorichard\n",
    "\n",
    "Kaggle private scoreboard snapshot: 0.45021\n",
    "\n",
    "[Snapshot](screen_kaggle_competition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 1 - TAKE HOME EXERCICES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "train_count = count_vect.fit_transform(train_df.text)  \n",
    "\n",
    "test_count = count_vect.fit_transform(test_df.text)\n",
    "\n",
    "train_words = count_vect.get_feature_names();    \n",
    "train_frequencies = train_count.toarray().sum(axis=0)  \n",
    "test_words = count_vect.get_feature_names();    \n",
    "test_frequencies = test_count.toarray().sum(axis=0)   \n",
    "\n",
    "d_train = dict(zip(train_words,train_frequencies))\n",
    "d_test = dict(zip(test_words,test_frequencies))\n",
    "\n",
    "train_result = sorted(d_train.items(), key=lambda d_train: d_train[1], reverse=True) \n",
    "test_result = sorted(d_test.items(), key=lambda d_test: d_test[1], reverse=True) \n",
    "\n",
    "train_sort_features = []\n",
    "train_sort_frequencies = []\n",
    "test_sort_features = []\n",
    "test_sort_frequencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_result:\n",
    "    train_sort_features.append(i[0])\n",
    "    train_sort_frequencies.append(i[1])\n",
    "    \n",
    "for i in test_result:\n",
    "    test_sort_features.append(i[0])\n",
    "    test_sort_frequencies.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 30 of words frequency for the training set \n",
    "plt.subplots(figsize=(90, 10))\n",
    "plt.title('Word frequency for top 30 words in Train dataset')\n",
    "\n",
    "g = sns.barplot(x= train_sort_features[:30], \n",
    "            y=train_sort_frequencies[:30])\n",
    "g.set_xticklabels(train_sort_features[:30], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 30 of words frequency for the testing set \n",
    "plt.subplots(figsize=(90, 10))\n",
    "plt.title('Word frequency for top 30 words in Test dataset')\n",
    "\n",
    "g = sns.barplot(x= test_sort_features[:30], \n",
    "            y=test_sort_frequencies[:30])\n",
    "g.set_xticklabels(test_sort_features[:30], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the TF-IDF vectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "#generation of the TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "#get feature names\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names()\n",
    "feature_names_tfidf[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The classifier made a total of 460 predictions\n",
    "- Out of those 460 cases, the classifier predicted \"sadness\" 74 times, \"joy\" 79 times, \"fear\" 110 times and \"anger\" 84 times \n",
    "- In reality, 69 tweets are \"sadness\", 81 \"joy\", 113 \"fear\" and 84 \"anger\"\n",
    "- The biggest values are on the diagonal and represent the true positives (TP) ie tweets which are categorized by \"anger\", \"fear\", \"joy\", \"sadness\" and which belonging at these categories\n",
    "- So the aim is to get values as important as possible on this diagonal matrix  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create of a multinomial classifier and train it using the training sets\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#predict Outputs\n",
    "y_train_pred_bayes = model.predict(X_train)\n",
    "y_test_pred_bayes = model.predict(X_test)\n",
    "\n",
    "print('Predicted value: ', np.mean(y_test_pred_bayes == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation confusion matrix \n",
    "cm_bayes = confusion_matrix(y_test, y_test_pred_bayes)\n",
    "print(cm_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get accuracy\n",
    "acc_train_bayes = accuracy_score(y_true=y_train, y_pred=y_train_pred_bayes)\n",
    "acc_test_bayes = accuracy_score(y_true=y_test, y_pred=y_test_pred_bayes)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision, recall, f1-score\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm_bayes, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort() #sort() method sorts the elements of a given list in a specific order \n",
    "    tick_marks = np.arange(len(classes)) #returns a ndarray object containing evenly spaced values within the given range   \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm_bayes, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm_bayes.shape[1]),\n",
    "           yticks=np.arange(cm_bayes.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm_bayes.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_bayes.shape[0]), range(cm_bayes.shape[1])):\n",
    "        plt.text(j, i, format(cm_bayes[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm_bayes[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "plot_confusion_matrix(cm_bayes, classes=my_tags, title='Confusion matrix')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences.\n",
    "\n",
    "- F-measure is the harmonic mean of precision and recall: it is better with Bayes in terms of precision and recall\n",
    "- The accuracy is the same \n",
    "- More true positif with Bayes - lesser overfitting with Bayes\n",
    "- Decision tree is the fastest because there is no calculation in its classification whereas there is some in Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Model\n",
    "fx = sns.lineplot(x = training_log['epoch'], y = training_log['accuracy'], data = training_log, label = 'train_accuracy', color='blue')\n",
    "fx = sns.lineplot(x = training_log['epoch'], y = training_log['val_accuracy'], data = training_log, label = 'val_accuracy', color='red')\n",
    "\n",
    "plt.title('Training Accuracy per epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper right')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#Loss Model\n",
    "fx = sns.lineplot(x=training_log['epoch'], y=training_log['loss'], data=training_log, label='train_loss', color='blue')\n",
    "fx = sns.lineplot(x=training_log['epoch'], y=training_log['val_loss'], data=training_log, label='val_loss', color='red')\n",
    "\n",
    "plt.title('Training loss per epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to this graphs, we can see the phenomenon of overfitting. \n",
    "Indeed, the accuracy and loss values are quite similar up to epoch 2. We then observe that the loss values of the valide set increase while those of the training test decrease.\n",
    "\n",
    "Concerning the accuracy, it is quite stable in both cases but higher in the case of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had learning words from their context by generating their vector. Now, we want to represent the sentence data for traiuning our model.\n",
    "The aim will then to get the vector of each sentence.\n",
    "\n",
    "They are different ways to have it:\n",
    "    - Using Doc2vec to create a numeric representation of a document (a document vector and in our case, a sentence vector)\n",
    "    - Doing the average of Word2Vec vectors (but we risk to lose the word order and thus fail to recognize many sophisticated linguistic phenomena. This method is therefore not suited for sentiment analysis tasks.\n",
    "    - Socher et al's approach: using matrix-vector operations for combining word vectors in an order given by a parse tree of a sentence. Depends on parsing so it could be suited for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 15 #define the number of words to display\n",
    "\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "sad_words = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \n",
    "fear_words = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('sad_words: ', sad_words)\n",
    "print('mining_words: ', fear_words)\n",
    "\n",
    "target_words = happy_words + angry_words + sad_words + fear_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(9, 8), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2 - KAGGLE COMPETITION: Twitter Emotion Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 3 - REPORT: Work developping the model for the competition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import keras\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A: Preparing the training and testing sets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some Data Preparation and Transformation, I got two datasets: train_df and test_df, which I loaded into two separate pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our datasets stored in pickle files\n",
    "train_df = pd.read_pickle(\"../DM19-Lab2-Homework/train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"../DM19-Lab2-Homework/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there were missing values\n",
    "#train_df\n",
    "train_miss_values = train_df.isnull()\n",
    "train_miss_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the 3 tweets in train_df with missing values\n",
    "train_df = train_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-check \n",
    "train_miss_values = train_df.isnull()\n",
    "train_miss_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df \n",
    "test_miss_values = test_df.isnull()\n",
    "test_miss_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there were duplicates values\n",
    "#train_df\n",
    "sum(train_df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df\n",
    "sum(test_df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B: Pre-processing steps*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was the Text Preprocessing part. These steps were important because they allow to detect, correct and remove corrupt or irrelevant records from our datasets. \n",
    "Thus, I decided to:\n",
    "- Remove Twitter handles\n",
    "- Remove some special characters, numbers and punctuations: remove links and usernames \n",
    "- Remove repetitions: remove char repetitions (e.g. whaaaaaat => what)\n",
    "- Lowercast: convert all characters from the tweet to lowercase\n",
    "- Transform the different short negation forms\n",
    "- Remove stop words: remove common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt) \n",
    "    return input_txt  \n",
    "\n",
    "#remove twitter handles (@user)\n",
    "train_df['tweet'] = np.vectorize(remove_pattern)(train_df['tweet'], \"@[\\w]*\")\n",
    "\n",
    "# remove special characters, numbers, punctuations\n",
    "train_df['tweet'] = train_df['tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "#remove repetitions\n",
    "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "train_df['tweet'] = train_df['tweet'].str.replace(pattern, r\"\\1\")\n",
    "\n",
    "#lowercasing\n",
    "train_df['tweet'] = train_df['tweet'].str.lower()\n",
    "\n",
    "#transform short negation form\n",
    "train_df['tweet'] = train_df['tweet'].str.replace(r\"(can't|cannot)\", 'can not')\n",
    "train_df['tweet'] = train_df['tweet'].str.replace(r\"n't\", ' not')\n",
    "\n",
    "#remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('nor')\n",
    "stopwords.remove('no')\n",
    "train_df['tweet'] = train_df['tweet'].apply(\n",
    "  lambda x: ' '.join([word for word in x.split() if word not in stopwords])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C: LR Model* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I proceeded to the main and final task: build a model to predict the emotion of a tweet. I used first a Logistic Regression Method to classify the tweets into the selected labels. It is a Machine Learning classification algorithm that is usually used to predict the probability of a categorical dependent variable. I chose an embedding using the TF-IDF vectorizer to feed the data (to convert string data into numerical data) into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation and fit and transform of X_train, X_test, y_train and y_test\n",
    "tknzr = TweetTokenizer(preserve_case=False)\n",
    "tfidf = TfidfVectorizer(max_features=20000, stop_words='english',\n",
    "                                     tokenizer=tknzr.tokenize)\n",
    "tfidf.fit(train_df['tweet'])\n",
    "\n",
    "X_train = tfidf.transform(train_df['tweet'])\n",
    "X_test = tfidf.transform(test_df['tweet'])\n",
    "y_train = train_df['emotion']\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build and run the LR model\n",
    "lr = LogisticRegression(C=6,n_jobs=-1,max_iter=1000)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting emotions from X_test thanks to lr method\n",
    "pred_result_lr = lr.predict(X_test)\n",
    "pred_result_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['emotion']=pred_result_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values \n",
    "x = test_df['emotion'].isnull()\n",
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicate values\n",
    "sum(test_df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results in a file\n",
    "test_df.to_csv('y_test_lr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a Kaggle score of 0.44752 and decided to use another model to get a greater accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D: LSTM Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on different observations and results that I read online, I decided thanks to this article:  http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/ to use then the LTSM Model. Indeed, they have compared the performance of different models in « Sentiment Analysis of Twitter data ». They proved that LTSM get a good accuracy (72,5%) with an implementation quite simple. LSTM is a model often used in sentiment analysis, partly solving the problems encountered by RNN thanks to gates which are able to control the memorizing process and decide what information to store in long term memory and what to get rid of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement LSTM for Sentimental Analysis, the first step was to vectorize the tweets and convert them into sequences of integers so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 300\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(train_df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.2: Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tok.texts_to_sequences(train_df['tweet'])\n",
    "test_seq = tok.texts_to_sequences(test_df['tweet'])\n",
    "\n",
    "\n",
    "train_seq_mat = sequence.pad_sequences(train_seq,maxlen=max_len)\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "\n",
    "print(train_seq_mat.shape)\n",
    "print(test_seq_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.3: Dealing with categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with label (string -> one-hot)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train) #.fit() finds the coefficients for the equation specified via the algorithm being used\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.4: Design the RNN model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined first the input and output shape which we needed later to build the LSTM model.\n",
    "\n",
    "Then, I build the LSTM network. As hyper parameters, I used in our case an embedded dimension equal to 128 and a lstm out equal to 128 too but their values are somehow intuitive as the value of dropout and batch_size. I used also softmax as activation function because I worked with categorical crossentropy and softmax appeared as the right activation method for that.\n",
    "\n",
    "To summarize, our input is a sequence of words of maximum length equal to max_words, our output is a binary sentiment label (0 or 1) and our model is a simple RNN model with one embedding, one LSTM, one dense layers and we had totally 2 709 256 parameters needed to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##build the LSTM model\n",
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "\n",
    "layer = Embedding(max_words+1,128,input_length=max_len)(inputs)\n",
    "layer = LSTM(128)(layer)\n",
    "layer = Dense(128,activation=\"relu\",name=\"FC1\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(output_shape,activation=\"softmax\",name=\"FC2\")(layer)\n",
    "\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.5: Train and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I fitted my model on training set and check the accuracy on validation set. I fixed first an epoch number equal to 3, the accuracy was then not so good with a score around 0.60. I then decided to change the number of epochs to 10 based again on the searches from the article mentioned above. However, I realized that the training time was going to be too long and I was not able to leave my computer at home to let the code run, so I used the trained model with 3 epochs knowing that the accuracy was much worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(train_seq_mat,y_train,batch_size=128,epochs=3,\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*E: Testing our Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally we tested our model on the testing set. After running the model on the data this is what it returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting emotions from X_test thanks to LSTM model\n",
    "pred_result_lstm = label_decode(label_encoder, model.predict(test_seq_mat, batch_size=128))\n",
    "test_df['emotion']= pred_result_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "test_df.head()\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values \n",
    "test_miss_values = test_df.isnull()\n",
    "test_miss_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform test_df in the Kaggle shape required \n",
    "#test_df.drop(columns=['tweet'],inplace=True)\n",
    "#test_df.index.rename('id',inplace=True)\n",
    "#test_df.rename(columns = {'tweet_id':'id', inplace = True) \n",
    "#test_df.set_index('id', inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I finally got a higher Kaggle score: 0.45021 by using the LSTM model while knowing that the accuracy could have been much higher by letting the model train in 10 epochs and not 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F: Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving result in a csv file\n",
    "test_df.to_csv('y_test_lstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
